{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model_pipeline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, tag_map = get_vocab('dataset/words.txt', 'dataset/tags.txt')\n",
    "t_sentences, t_labels, _ = get_params(vocab, tag_map, 'dataset/train/sentences.txt', 'dataset/train/labels.txt')\n",
    "v_sentences, v_labels, _ = get_params(vocab, tag_map, 'dataset/val/sentences.txt', 'dataset/val/labels.txt')\n",
    "test_sentences, test_labels, _ = get_params(vocab, tag_map, 'dataset/test/sentences.txt', 'dataset/test/labels.txt')\n",
    "\n",
    "# dataset usefull info\n",
    "vocab_size = len(vocab) # dataset vocab\n",
    "embedded_size = len(t_sentences[0]) # words/sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the LSTM model\n",
    "model = tl.Serial(\n",
    "    tl.Embedding(vocab_size, embedded_size), # Embedding layer\n",
    "    tl.LSTM(embedded_size), # LSTM layer\n",
    "    tl.Dense(len(tag_map)), # Dense layer with len(tag_map) units\n",
    "    tl.LogSoftmax()  # LogSoftmax layer\n",
    ")\n",
    "# display the model\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "/home/asus-g10ac/Downloads/soccer-net/venv/lib/python3.6/site-packages/jax/lib/xla_bridge.py:387: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
      "  \"jax.host_count has been renamed to jax.process_count. This alias \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step      1: Total number of trainable weights: 255056\n",
      "Step      1: Ran 1 train steps in 2.61 secs\n",
      "Step      1: train CrossEntropyLoss |  2.02989888\n",
      "Step      1: eval  CrossEntropyLoss |  0.89070380\n",
      "Step      1: eval          Accuracy |  0.93600001\n",
      "\n",
      "Step    100: Ran 99 train steps in 2.19 secs\n",
      "Step    100: train CrossEntropyLoss |  0.09123459\n",
      "Step    100: eval  CrossEntropyLoss |  0.12652491\n",
      "Step    100: eval          Accuracy |  0.98200001\n",
      "\n",
      "Step    200: Ran 100 train steps in 2.09 secs\n",
      "Step    200: train CrossEntropyLoss |  0.00685188\n",
      "Step    200: eval  CrossEntropyLoss |  0.14044195\n",
      "Step    200: eval          Accuracy |  0.98196875\n",
      "\n",
      "Step    300: Ran 100 train steps in 2.11 secs\n",
      "Step    300: train CrossEntropyLoss |  0.00282311\n",
      "Step    300: eval  CrossEntropyLoss |  0.15059000\n",
      "Step    300: eval          Accuracy |  0.98203125\n",
      "\n",
      "Step    400: Ran 100 train steps in 2.08 secs\n",
      "Step    400: train CrossEntropyLoss |  0.00138004\n",
      "Step    400: eval  CrossEntropyLoss |  0.15800631\n",
      "Step    400: eval          Accuracy |  0.98203126\n",
      "\n",
      "Step    500: Ran 100 train steps in 2.12 secs\n",
      "Step    500: train CrossEntropyLoss |  0.00080818\n",
      "Step    500: eval  CrossEntropyLoss |  0.16410935\n",
      "Step    500: eval          Accuracy |  0.98190625\n",
      "\n",
      "Step    600: Ran 100 train steps in 2.07 secs\n",
      "Step    600: train CrossEntropyLoss |  0.00052289\n",
      "Step    600: eval  CrossEntropyLoss |  0.16695655\n",
      "Step    600: eval          Accuracy |  0.98203126\n",
      "\n",
      "Step    700: Ran 100 train steps in 2.10 secs\n",
      "Step    700: train CrossEntropyLoss |  0.00036824\n",
      "Step    700: eval  CrossEntropyLoss |  0.17059190\n",
      "Step    700: eval          Accuracy |  0.98200001\n",
      "\n",
      "Step    800: Ran 100 train steps in 2.11 secs\n",
      "Step    800: train CrossEntropyLoss |  0.00028310\n",
      "Step    800: eval  CrossEntropyLoss |  0.17456104\n",
      "Step    800: eval          Accuracy |  0.98193750\n",
      "\n",
      "Step    900: Ran 100 train steps in 2.17 secs\n",
      "Step    900: train CrossEntropyLoss |  0.00022613\n",
      "Step    900: eval  CrossEntropyLoss |  0.17566083\n",
      "Step    900: eval          Accuracy |  0.98206251\n",
      "\n",
      "Step   1000: Ran 100 train steps in 2.13 secs\n",
      "Step   1000: train CrossEntropyLoss |  0.00018471\n",
      "Step   1000: eval  CrossEntropyLoss |  0.17911372\n",
      "Step   1000: eval          Accuracy |  0.98196875\n"
     ]
    }
   ],
   "source": [
    "# remove model path if it exists\n",
    "if os.path.exists('model'):\n",
    "    shutil.rmtree('model')\n",
    "\n",
    "# Create training data generator\n",
    "train_generator = trax.data.inputs.add_loss_weights(\n",
    "    data_generator(batch_size, t_sentences, t_labels, vocab['<PAD>'], True),\n",
    "    id_to_mask=vocab['<PAD>']\n",
    ")\n",
    "\n",
    "# Create validation data generator\n",
    "eval_generator = trax.data.inputs.add_loss_weights(\n",
    "    data_generator(batch_size, v_sentences, v_labels, vocab['<PAD>'], True),\n",
    "    id_to_mask=vocab['<PAD>']\n",
    ")\n",
    "\n",
    "# initialize the training loop\n",
    "training_loop = train_model(model, train_generator, eval_generator, train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel = tl.Serial(\\n    tl.Embedding(vocab_size, embedded_size),    # Embedding layer\\n    tl.LSTM(embedded_size),                     # LSTM layer\\n    tl.Dense(len(tags)),                        # Dense layer with len(tags) units\\n    tl.LogSoftmax()                             # LogSoftmax layer\\n)\\nmodel.init(trax.shapes.ShapeDtype((1, 1), dtype=np.int32))\\nmodel.init_from_file('model/model.pkl.gz', weights_only=True)\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pretrained model\n",
    "\"\"\"\n",
    "model = tl.Serial(\n",
    "    tl.Embedding(vocab_size, embedded_size),    # Embedding layer\n",
    "    tl.LSTM(embedded_size),                     # LSTM layer\n",
    "    tl.Dense(len(tags)),                        # Dense layer with len(tags) units\n",
    "    tl.LogSoftmax()                             # LogSoftmax layer\n",
    ")\n",
    "model.init(trax.shapes.ShapeDtype((1, 1), dtype=np.int32))\n",
    "model.init_from_file('model/model.pkl.gz', weights_only=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model with evaluation data\n",
    "eval_model_gen = data_generator(len(test_sentences), test_sentences, test_labels, vocab['<PAD>'], shuffle=True)\n",
    "x, y = next(eval_model_gen)\n",
    "# print(\"input shapes\", x.shape, y.shape)\n",
    "\n",
    "# Evaluate modle accuracy\n",
    "#print(f\"accuracy: {evaluate_prediction(model(x), y, vocab['<PAD>'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your own data\n",
    "#txts = \"today we are in spain for the big final match between the cristiano ronaldo team real madrid and barcelona and we hope to see a really good football match\"\n",
    "#txts_token = [vocab[token] if token in vocab else vocab['UNK'] for token in txts.split(' ')]\n",
    "#eval_model_gen = data_generator(1, [txts_token], [[0 for _ in range(len(txts_token))]], vocab['<PAD>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: today we are in spain for the big final match between the cristiano ronaldo team real madrid and barcelona and we hope to see a really good football match\n",
      "\n",
      "Preds: [('spain', 0, 4, 'O', 'TEA'), ('cristiano', 0, 1, 'O', 'PLA'), ('ronaldo', 0, 1, 'O', 'PLA'), ('madrid', 0, 4, 'O', 'TEA'), ('barcelona', 0, 4, 'O', 'TEA')]\n"
     ]
    }
   ],
   "source": [
    "# create the evaluation inputs\n",
    "x, y = next(eval_model_gen)\n",
    "# print(\"input shapes\", x.shape, y.shape, x[0])\n",
    "\n",
    "# sample prediction\n",
    "tmp_pred = model(x)\n",
    "# print(type(tmp_pred))\n",
    "# print(f\"tmp_pred has shape: {tmp_pred.shape}\")\n",
    "\n",
    "x_aux, inv_vocab = [], {v: k for k, v in vocab.items()}\n",
    "for idx, p in enumerate(x[:1]):\n",
    "    for t in p:\n",
    "        for k,v in vocab.items():\n",
    "            if v == t:\n",
    "                x_aux.append(k)\n",
    "    preds = [(t,p1,p2,[q for q,w in tag_map.items() if w == p1][0],[q for q,w in tag_map.items() if w == p2][0]) for (t,p1,p2) in list(zip(x_aux, y[idx], np.argmax(tmp_pred, axis=2)[idx])) if p2 != 0]\n",
    "    \n",
    "    print(f\"Sentence: {' '.join([inv_vocab[t] for t in x[0]])}\\n\")\n",
    "    print(f'Preds: {preds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "adcc4e0c82088ed9b5b31e15a41bab8df577002c046d5296f07b2261291f413e"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 ('venv': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
