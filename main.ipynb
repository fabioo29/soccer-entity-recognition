{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import trax \n",
    "import shutil\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from trax import layers as tl\n",
    "from trax.supervised import training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd.seed(33)\n",
    "batch_size = 64\n",
    "train_steps = 1000\n",
    "vocab_size = None\n",
    "d_model = None\n",
    "tags = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(vocab_path, tags_path):\n",
    "    vocab = {}\n",
    "    with open(vocab_path) as f:\n",
    "        for i, l in enumerate(f.read().splitlines()):\n",
    "            vocab[l] = i  # to avoid the 0\n",
    "        # loading tags (we require this to map tags to their indices)\n",
    "    vocab['<PAD>'] = len(vocab) # 35180\n",
    "    tag_map = {}\n",
    "    with open(tags_path) as f:\n",
    "        for i, t in enumerate(f.read().splitlines()):\n",
    "            tag_map[t] = i \n",
    "    \n",
    "    return vocab, tag_map\n",
    "\n",
    "def get_params(vocab, tag_map, sentences_file, labels_file):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    with open(sentences_file) as f:\n",
    "        for sentence in f.read().splitlines():\n",
    "            # replace each token by its index if it is in vocab\n",
    "            # else use index of UNK_WORD\n",
    "            s = [vocab[token] if token in vocab else vocab['UNK'] for token in sentence.split(' ')]\n",
    "            sentences.append(s)\n",
    "\n",
    "    with open(labels_file) as f:\n",
    "        for sentence in f.read().splitlines():\n",
    "            # replace each label by its index\n",
    "            l = [tag_map[label] for label in sentence.split()] # I added plus 1 here\n",
    "            labels.append(l) \n",
    "    return sentences, labels, len(sentences)\n",
    "\n",
    "def data_generator(batch_size, x, y, pad, shuffle=False, verbose=False):\n",
    "    '''\n",
    "      Input: \n",
    "        batch_size - integer describing the batch size\n",
    "        x - list containing sentences where words are represented as integers\n",
    "        y - list containing tags associated with the sentences\n",
    "        shuffle - Shuffle the data order\n",
    "        pad - an integer representing a pad character\n",
    "        verbose - Print information during runtime\n",
    "      Output:\n",
    "        a tuple containing 2 elements:\n",
    "        X - np.ndarray of dim (batch_size, max_len) of padded sentences\n",
    "        Y - np.ndarray of dim (batch_size, max_len) of tags associated with the sentences in X\n",
    "    '''\n",
    "    \n",
    "    # count the number of lines in data_lines\n",
    "    num_lines = len(x)\n",
    "    \n",
    "    # create an array with the indexes of data_lines that can be shuffled\n",
    "    lines_index = [*range(num_lines)]\n",
    "    \n",
    "    # shuffle the indexes if shuffle is set to True\n",
    "    if shuffle:\n",
    "        rnd.shuffle(lines_index)\n",
    "    \n",
    "    index = 0 # tracks current location in x, y\n",
    "    while True:\n",
    "        buffer_x = [0] * batch_size # Temporal array to store the raw x data for this batch\n",
    "        buffer_y = [0] * batch_size # Temporal array to store the raw y data for this batch\n",
    "        \n",
    "        # Copy into the temporal buffers the sentences in x[index : index + batch_size] \n",
    "        # along with their corresponding labels y[index : index + batch_size]\n",
    "        # Find maximum length of sentences in x[index : index + batch_size] for this batch. \n",
    "        # Reset the index if we reach the end of the data set, and shuffle the indexes if needed.\n",
    "        max_len = 0\n",
    "        for i in range(batch_size):\n",
    "             # if the index is greater than or equal to the number of lines in x\n",
    "            if index >= num_lines:\n",
    "                # then reset the index to 0\n",
    "                index = 0\n",
    "                # re-shuffle the indexes if shuffle is set to True\n",
    "                if shuffle:\n",
    "                    rnd.shuffle(lines_index)\n",
    "            \n",
    "            # The current position is obtained using `lines_index[index]`\n",
    "            # Store the x value at the current position into the buffer_x\n",
    "            buffer_x[i] = x[lines_index[index]]\n",
    "            \n",
    "            # Store the y value at the current position into the buffer_y\n",
    "            buffer_y[i] = y[lines_index[index]]\n",
    "            \n",
    "            lenx = len(x[lines_index[index]])    #length of current x[]\n",
    "            if lenx > max_len:\n",
    "                max_len = lenx                   #max_len tracks longest x[]\n",
    "            \n",
    "            # increment index by one\n",
    "            index += 1\n",
    "\n",
    "\n",
    "        # create X,Y, NumPy arrays of size (batch_size, max_len) 'full' of pad value\n",
    "        X = np.full((batch_size, max_len), pad)\n",
    "        Y = np.full((batch_size, max_len), pad)\n",
    "\n",
    "        # copy values from lists to NumPy arrays. Use the buffered values\n",
    "        for i in range(batch_size):\n",
    "            # get the example (sentence as a tensor)\n",
    "            # in `buffer_x` at the `i` index\n",
    "            x_i = buffer_x[i]\n",
    "            \n",
    "            # similarly, get the example's labels\n",
    "            # in `buffer_y` at the `i` index\n",
    "            y_i = buffer_y[i]\n",
    "            \n",
    "            # Walk through each word in x_i\n",
    "            for j in range(len(x_i)):\n",
    "                # store the word in x_i at position j into X\n",
    "                X[i, j] = x_i[j]\n",
    "                \n",
    "                # store the label in y_i at position j into Y\n",
    "                Y[i, j] = y_i[j]\n",
    "\n",
    "        if verbose: print(\"index=\", index)\n",
    "        yield((X,Y))\n",
    "\n",
    "def train_model(NER, train_generator, eval_generator, train_steps=1, output_dir='model'):\n",
    "    '''\n",
    "    Input: \n",
    "        NER - the model you are building\n",
    "        train_generator - The data generator for training examples\n",
    "        eval_generator - The data generator for validation examples,\n",
    "        train_steps - number of training steps\n",
    "        output_dir - folder to save your model\n",
    "    Output:\n",
    "        training_loop - a trax supervised training Loop\n",
    "    '''\n",
    "    train_task = training.TrainTask(\n",
    "        train_generator, # A train data generator\n",
    "        loss_layer = tl.CrossEntropyLoss(), # A cross-entropy loss function\n",
    "        optimizer = trax.optimizers.Adam(0.01),  # The adam optimizer\n",
    "    )\n",
    "\n",
    "    eval_task = training.EvalTask(\n",
    "      labeled_data = eval_generator, # A labeled data generator\n",
    "      metrics = [tl.CrossEntropyLoss(), tl.Accuracy()], # Evaluate with cross-entropy loss and accuracy\n",
    "      n_eval_batches = 10  # Number of batches to use on each evaluation\n",
    "    )\n",
    "\n",
    "    training_loop = training.Loop(\n",
    "        NER, # A model to train\n",
    "        train_task, # A train task\n",
    "        eval_tasks = [eval_task], # The evaluation task\n",
    "        output_dir = output_dir) # The output directory\n",
    "\n",
    "    # Train with train_steps\n",
    "    training_loop.run(n_steps = train_steps)\n",
    "    return training_loop\n",
    "\n",
    "def evaluate_prediction(pred, labels, pad):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        pred: prediction array with shape \n",
    "            (num examples, max sentence length in batch, num of classes)\n",
    "        labels: array of size (batch_size, seq_len)\n",
    "        pad: integer representing pad character\n",
    "    Outputs:\n",
    "        accuracy: float\n",
    "    \"\"\"\n",
    "    outputs = np.argmax(pred, axis=2)\n",
    "    print(\"outputs shape:\", outputs.shape)\n",
    "\n",
    "    mask = labels != pad\n",
    "    print(\"mask shape:\", mask.shape, \"mask[0][20:30]:\", mask[0][20:30])\n",
    "\n",
    "    accuracy = np.sum(outputs == labels) / float(np.sum(mask))\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def predict(sentence, model, vocab, tag_map):\n",
    "    # create tensor with info about each word in the sentence\n",
    "    s = [vocab[token] if token in vocab else vocab['UNK'] for token in sentence.replace('\\n', '').split(' ')]\n",
    "    batch_data = np.ones((1, len(s)))\n",
    "    batch_data[0][:] = s\n",
    "    sentence = np.array(batch_data).astype(int)\n",
    "\n",
    "    # predict the tags for each word\n",
    "    output = model(sentence)\n",
    "    \n",
    "    # for each word get the label with the max value\n",
    "    outputs = np.argmax(output, axis=2)\n",
    "\n",
    "    # convert each label (number) to its string value\n",
    "    labels = list(tag_map.keys())\n",
    "    pred = []\n",
    "    for i in range(len(outputs[0])):\n",
    "        idx = outputs[0][i] \n",
    "        pred_label = labels[idx]\n",
    "        pred.append(pred_label)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, tag_map = get_vocab('dataset/words.txt', 'dataset/tags.txt')\n",
    "t_sentences, t_labels, t_size = get_params(vocab, tag_map, 'dataset/train/sentences.txt', 'dataset/train/labels.txt')\n",
    "v_sentences, v_labels, v_size = get_params(vocab, tag_map, 'dataset/val/sentences.txt', 'dataset/val/labels.txt')\n",
    "test_sentences, test_labels, test_size = get_params(vocab, tag_map, 'dataset/test/sentences.txt', 'dataset/test/labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial[\n",
      "  Embedding_5292_30\n",
      "  LSTM_30\n",
      "  Dense_6\n",
      "  LogSoftmax\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedded_size = len(t_sentences[0])\n",
    "tags = tag_map\n",
    "\n",
    "# initializing your model\n",
    "model = tl.Serial(\n",
    "    tl.Embedding(vocab_size, embedded_size), # Embedding layer\n",
    "    tl.LSTM(embedded_size), # LSTM layer\n",
    "    tl.Dense(len(tags)), # Dense layer with len(tags) units\n",
    "    tl.LogSoftmax()  # LogSoftmax layer\n",
    ")\n",
    "# display your model\n",
    "print(model)\n",
    "\n",
    "if os.path.exists('model'):\n",
    "    shutil.rmtree('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data, mask pad id=35180 for training.\n",
    "train_generator = trax.data.inputs.add_loss_weights(\n",
    "    data_generator(batch_size, t_sentences, t_labels, vocab['<PAD>'], True),\n",
    "    id_to_mask=vocab['<PAD>'])\n",
    "\n",
    "# Create validation data, mask pad id=35180 for training.\n",
    "eval_generator = trax.data.inputs.add_loss_weights(\n",
    "    data_generator(batch_size, v_sentences, v_labels, vocab['<PAD>'], True),\n",
    "    id_to_mask=vocab['<PAD>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "/home/asus-g10ac/Downloads/soccer-net/venv/lib/python3.6/site-packages/jax/lib/xla_bridge.py:387: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
      "  \"jax.host_count has been renamed to jax.process_count. This alias \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step      1: Total number of trainable weights: 166266\n",
      "Step      1: Ran 1 train steps in 3.05 secs\n",
      "Step      1: train CrossEntropyLoss |  2.56266379\n",
      "Step      1: eval  CrossEntropyLoss |  1.94174962\n",
      "Step      1: eval          Accuracy |  0.06401042\n",
      "\n",
      "Step    100: Ran 99 train steps in 0.91 secs\n",
      "Step    100: train CrossEntropyLoss |  0.19188018\n",
      "Step    100: eval  CrossEntropyLoss |  0.06698018\n",
      "Step    100: eval          Accuracy |  0.98890624\n",
      "\n",
      "Step    200: Ran 100 train steps in 0.93 secs\n",
      "Step    200: train CrossEntropyLoss |  0.01459531\n",
      "Step    200: eval  CrossEntropyLoss |  0.06450000\n",
      "Step    200: eval          Accuracy |  0.98895832\n",
      "\n",
      "Step    300: Ran 100 train steps in 0.92 secs\n",
      "Step    300: train CrossEntropyLoss |  0.00617506\n",
      "Step    300: eval  CrossEntropyLoss |  0.06401399\n",
      "Step    300: eval          Accuracy |  0.98890625\n",
      "\n",
      "Step    400: Ran 100 train steps in 0.92 secs\n",
      "Step    400: train CrossEntropyLoss |  0.00324368\n",
      "Step    400: eval  CrossEntropyLoss |  0.06433085\n",
      "Step    400: eval          Accuracy |  0.98880208\n",
      "\n",
      "Step    500: Ran 100 train steps in 0.93 secs\n",
      "Step    500: train CrossEntropyLoss |  0.00185992\n",
      "Step    500: eval  CrossEntropyLoss |  0.06541066\n",
      "Step    500: eval          Accuracy |  0.98609375\n",
      "\n",
      "Step    600: Ran 100 train steps in 0.94 secs\n",
      "Step    600: train CrossEntropyLoss |  0.00119611\n",
      "Step    600: eval  CrossEntropyLoss |  0.06590341\n",
      "Step    600: eval          Accuracy |  0.98885416\n",
      "\n",
      "Step    700: Ran 100 train steps in 0.97 secs\n",
      "Step    700: train CrossEntropyLoss |  0.00084980\n",
      "Step    700: eval  CrossEntropyLoss |  0.06674873\n",
      "Step    700: eval          Accuracy |  0.98895832\n",
      "\n",
      "Step    800: Ran 100 train steps in 0.95 secs\n",
      "Step    800: train CrossEntropyLoss |  0.00064232\n",
      "Step    800: eval  CrossEntropyLoss |  0.06872251\n",
      "Step    800: eval          Accuracy |  0.98880208\n",
      "\n",
      "Step    900: Ran 100 train steps in 0.91 secs\n",
      "Step    900: train CrossEntropyLoss |  0.00050573\n",
      "Step    900: eval  CrossEntropyLoss |  0.06877638\n",
      "Step    900: eval          Accuracy |  0.98614584\n",
      "\n",
      "Step   1000: Ran 100 train steps in 0.94 secs\n",
      "Step   1000: train CrossEntropyLoss |  0.00040989\n",
      "Step   1000: eval  CrossEntropyLoss |  0.06975083\n",
      "Step   1000: eval          Accuracy |  0.98614584\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "training_loop = train_model(model, train_generator, eval_generator, train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel = tl.Serial(\\n    tl.Embedding(vocab_size, embedded_size),    # Embedding layer\\n    tl.LSTM(embedded_size),                     # LSTM layer\\n    tl.Dense(len(tags)),                        # Dense layer with len(tags) units\\n    tl.LogSoftmax()                             # LogSoftmax layer\\n)\\nmodel.init(trax.shapes.ShapeDtype((1, 1), dtype=np.int32))\\nmodel.init_from_file('model/model.pkl.gz', weights_only=True)\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "model = tl.Serial(\n",
    "    tl.Embedding(vocab_size, embedded_size),    # Embedding layer\n",
    "    tl.LSTM(embedded_size),                     # LSTM layer\n",
    "    tl.Dense(len(tags)),                        # Dense layer with len(tags) units\n",
    "    tl.LogSoftmax()                             # LogSoftmax layer\n",
    ")\n",
    "model.init(trax.shapes.ShapeDtype((1, 1), dtype=np.int32))\n",
    "model.init_from_file('model/model.pkl.gz', weights_only=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model_gen = data_generator(len(t_sentences), test_sentences, test_labels, vocab['<PAD>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds: [('ronaldo', 1, 1, 'PLA', 'PLA')]\n"
     ]
    }
   ],
   "source": [
    "eval_model_gen = data_generator(1, [test_sentences[3]], [test_labels[3]], vocab['<PAD>'])\n",
    "\n",
    "# create the evaluation inputs\n",
    "x, y = next(eval_model_gen)\n",
    "# print(\"input shapes\", x.shape, y.shape, x[0])\n",
    "\n",
    "# sample prediction\n",
    "tmp_pred = model(x)\n",
    "# print(type(tmp_pred))\n",
    "# print(f\"tmp_pred has shape: {tmp_pred.shape}\")\n",
    "\n",
    "x_aux = []\n",
    "for idx, p in enumerate(x[:5]):\n",
    "    for t in p:\n",
    "        for k,v in vocab.items():\n",
    "            if v == t:\n",
    "                x_aux.append(k)\n",
    "    \n",
    "    print('Preds:', [(t,p1,p2,[q for q,w in tags.items() if w == p1][0],[q for q,w in tags.items() if w == p2][0]) for (t,p1,p2) in list(zip(x_aux, y[idx], np.argmax(tmp_pred, axis=2)[idx])) if p2 != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs shape: (1, 30)\n",
      "mask shape: (1, 30) mask[0][20:30]: [ True  True  True  True  True  True  True  True  True  True]\n",
      "accuracy:  0.96666664\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_prediction(model(x), y, vocab['<PAD>'])\n",
    "print(\"accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('calm', 3109, 'calm', 'PLA')]\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer() # initialize nltk stemmer\n",
    "\n",
    "sentence = \"\"\"\n",
    "madrid starting lineup was the starting lineups brought to you by soccer calm take a look at the table right now and it is barcelona 76 points atletico winning pocket at 70 real madrid at 66 now again on paper it seems even with the defeat here today barcelona should\n",
    "\"\"\"\n",
    "\n",
    "sentence = [stemmer.stem(w) for w in word_tokenize(sentence)[:50]]\n",
    "\n",
    "xtt = []\n",
    "s = [vocab[token] if token in vocab else vocab['UNK'] for token in sentence]\n",
    "xtt.append(s)\n",
    "\n",
    "ytt = []\n",
    "l = [tag_map[label] for label in ' '.join(['O']*50).split()] # I added plus 1 here\n",
    "ytt.append(l)\n",
    "\n",
    "xt, yt = next(data_generator(len(xtt), xtt, ytt, vocab['<PAD>']))\n",
    "\n",
    "len(xtt), len(ytt), len(xtt[0]), len(ytt[0])\n",
    "\n",
    "predtt = model(x)\n",
    "\n",
    "s = [vocab[token] if token in vocab else vocab['UNK'] for token in sentence]\n",
    "t = [token if token in vocab else 'UNK' for token in sentence]\n",
    "\n",
    "# for each word get the label with the max value\n",
    "outputs = np.argmax(predtt, axis=2)\n",
    "\n",
    "# convert each label (number) to its string value\n",
    "labels = list(tag_map.keys())\n",
    "pred = []\n",
    "for i in range(len(outputs[0])):\n",
    "    idx = outputs[0][i] \n",
    "    pred_label = labels[idx]\n",
    "    pred.append(pred_label)\n",
    "\n",
    "print([(word,token,token_word,tag) for (word,token,token_word,tag) in list(zip(sentence, s, t, pred)) if tag != 'O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "adcc4e0c82088ed9b5b31e15a41bab8df577002c046d5296f07b2261291f413e"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 ('venv': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
